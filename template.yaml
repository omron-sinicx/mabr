theme: default # default || dark
organization: OMRON SINIC X
twitter: '@omron_sinicx'
title: 'Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for Cooperative Manipulation by Mobile Robots'
conference: IROS2024
resources:
  paper: https://arxiv.org/abs/2312.02008
  code: # TBD
  video: # TBD
  blog: https://medium.com/sinicx/multipolar-multi-source-policy-aggregation-for-transfer-reinforcement-learning-between-diverse-bc42a152b0f5
  huggingface: # NA
description: introduces the Multi-Agent Coordination Skill Database, allowing multiple mobile robots to efficiently use past memories to adapt to new tasks.
image: https://omron-sinicx.github.io/mabr/assets/teaser.png
url: https://omron-sinicx.github.io/mabr
speakerdeck: # TBD
authors:
  - name: So Kuroki
    affiliation: [1, 2]
    url: http://barekatain.me/
    position: intern
  - name: Mai Nishimura
    affiliation: [1]
    position: Senior Researcher
    url: https://denkiwakame.github.io
  - name: Tadashi Kozuno
    affiliation: [1]
    position: Senior Researcher
    url: https://tadashik.github.io/
contact_ids: ['omron', 2] #=> github issues, contact@sinicx.com, 2nd author
affiliations:
  - OMRON SINIC X Corporation
  - Technical University of Munich
meta:
  - '* work done as an intern at OMRON SINIC X.'
bibtex: >
  @inproceedings{kuroki2024iros,
    title={Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for Cooperative Push Manipulation by Mobile Robots},
    author={So Kuroki, Mai Nishiura, Tadashi Kozuno},
    booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    organization={IEEE}
    year={2024}
  }
header:
  bg_curve:
    sinic_curve.png
    #  bg_image: https://github.com/omron-sinicx/swarm-body/raw/main/images/teaser.jpg

teaser: teaser.png
overview: |
  Due to the complex interactions between agents, learning multi-agent control policy often requires a prohibited amount of data.
  This paper aims to enable multi-agent systems to effectively utilize past memories to adapt to novel collaborative tasks in a data-efficient fashion.
  We propose the *Multi-Agent Coordination Skill Database*, a repository for storing a collection of coordinated behaviors associated with key vectors distinctive to them. Our Transformer-based skill encoder effectively captures spatio-temporal interactions that contribute to coordination and provides a unique skill representation for each coordinated behavior. By leveraging only a small number of demonstrations of the target task, the database enables us to train the policy using a dataset augmented with the retrieved demonstrations. Experimental evaluations demonstrate that our method achieves a significantly higher success rate in push manipulation tasks compared with baseline methods like few-shot imitation learning. Furthermore, we validate the effectiveness of our retrieve-and-learn framework in a real environment using a team of wheeled robots.

body:
  - title: Multi-Agent Behavior Retrieval
    text: |
      ### Retrieve-and-Learn Framework
      Given a large task-agnostic prior dataset $\mathcal D_{\text{prior}}$ and a few demonstrations $\mathcal D_{\text{target}}$ collected from the target task, our main objective is to retrieve coordination skills from $\mathcal D_{\text{prior}}$ that facilitate downstream policy learning for the target task.

      <img src="assets/method.png" alt="" />

      Our retrieve-and-learn framework consists of three primary components: **(i) Database Construction**, **(ii) Coordination Skill Retrieval**,
      and **(iii) Retrieval-Augmented Policy Training**.

      #### Multi-Agent Coordination Skill Database

      $\mathcal D_{\text{prior}}$ consists of task-agnostic demonstrations of $N$ mobile agents.
      To effectively retrieve demonstrations from $D_{\text{prior}}$ that are relevant to $\mathcal D_{\text{target}}$, we seek an abstract representation to measure the similarity between the two multi-agent demonstrations. We refer to this as multi-agent coordination skill representation, a compressed vector representation that is distinctive to the specific coordination behavior.
      For $N$ agents‚Äô states $\mathbf s_t$ , we aim to learn a skill encoder $\mathcal E$ that maps the state representation of multiple agents into a single representative vector $\mathbf z \in \mathbb R^n$.

      For this, we introduce a Transformer-based coordination skill encoder, which *learns to capture interactions among agents as well as interactions between agents and a manipulation object*.
      <div class="uk-child-width-expand@s" uk-grid>
        <div class="">
          <img src="assets/skill_encoder.png" />
        </div>
        <div class="">
          To obtain predictable skill representation space, we train
          the Transformer-based skill encoder as a prediction model of
          the agent‚Äôs future trajectories i.e., actions.
          In our network, the past context encoder is comprised of stacked multi-head self-attention layers that learn to attend to past trajectories across spatial and temporal domains. The future trajectory decoder is composed of multi-head self-attention layers and subsequent stacked multi-head cross-attention layers that integrate the past trajectory information and input tokens.
        </div>
      </div>

      #### Retrieval-Augmented Policy Training
      We assume that the prior dataset $\mathcal D_{\text{prior}}$ and the target demonstrations $\mathcal D_{\text{target}}$ are composed as follows,

      |||
      |---|---|
      | $\mathcal D_{\text{prior}}$ | The prior dataset encompasses a large-scale offline demonstration of diverse cooperative tasks. The dataset also includes noisy or sub-optimal demonstrations to mimic real-world scenarios. All demonstrations are task-agnostic, i.e., each data is stored without any specific task annotations.|
      | $\mathcal D_{\text{target}}$ | The target dataset encompasses a small amount of expert data, i.e., all data is composed of wellcoordinated demonstrations to complete the target task.|

      Given $\mathcal D_{\text{prior}}$ and $\mathcal D_{\text{target}}$, we aim to retrieve cooperative behaviors similar to those seen in $\mathcal D_{\text{target}}$ in the learned coordination skill space. Once we obtain the retrieved data $\mathcal D_{\text{ret}}$, we train a multi-agent control policy using $\mathcal D_{\text{target}}$ augmented with $\mathcal D_{\text{ret}}$. That is, the training data $\mathcal D_{\text{train}}$ is described as $\mathcal D_{\text{train}} = \mathcal D_{\text{target}} \cup \mathcal D_{\text{ret}}$.

  - title: Results
    text: |
      ### Quantitative Results on Simulated Demonstrations
      We assume $N$ mobile robots are navigated to push an object toward a predefined goal state. To explore various coordination scenarios, we specifically focus on three different values of $N \in \{2, 3, 4\}$. For each of these setups, we collect demonstrations that involve four distinct tasks. These four tasks are carefully designed to encompass two different objects (**stick** or **block**) and manipulation difficulties (**easy** or **hard**).

      <div class="uk-overflow-auto">
        <table class="uk-table uk-table-small uk-text-small uk-table-divider">
          <thead>
            <tr>
              <th rowspan="2">Num of Agents</th>
              <th colspan="2" class="uk-text-center">task</th>
              <th colspan="3" class="uk-text-center">success rate ‚¨ÜÔ∏è [%]</th>
            </tr>
            <tr>
              <th>object</th>
              <th>level</th>
              <th>trajectory matching</th>
              <th>few-shot imitation learning</th>
              <th>‚ú®ours‚ú®</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>2</td>
              <td>üß±block<br/><br/>ü™Ñstick<br/><br/></td>
              <td><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span><br/><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span></td>
              <td><span class="uk-text-bold">41.4&plusmn;5.9</span><br/>55.7&plusmn;6.0<br/>32.9&plusmn;5.7<br/><span class="uk-text-bold">68.6&plusmn;5.6</span></td>
              <td>20.0&plusmn;4.8<br/>15.7&plusmn;4.4<br/>5.7&plusmn;2.8<br/>12.9&plusmn;4.0</td>
              <td><span class="uk-text-bold">41.4&plusmn;5.9<br/>57.1&plusmn;6.0<br/>35.7&plusmn;5.8</span><br/>67.1&plusmn;5.7</td>
            </tr>
            <tr>
              <td>3</td>
              <td>üß±block<br/><br/>ü™Ñstick<br/><br/></td>
              <td><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span><br/><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span></td>
              <td>32.9&plusmn;5.7<br/>34.3&plusmn;5.7<br/>34.3&plusmn;5.7<br/>52.9&plusmn;6.0</td>
              <td><span class="uk-text-bold">48.6&plusmn;6.0</span><br/>77.1&plusmn;5.1<br/>28.6&plusmn;5.4<br/><span class="uk-text-bold">67.1&plusmn;5.7</span></td>
              <td>42.9&plusmn;6.0<br/><span class="uk-text-bold">90.0&plusmn;3.6<br/>41.4&plusmn;5.9</span><br/>65.7&plusmn;5.7</td>
            </tr>
            <tr>
              <td>4</td>
              <td>üß±block<br/><br/>ü™Ñstick<br/><br/></td>
              <td><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span><br/><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span></td>
              <td>55.7&plusmn;6.0<br/>78.6&plusmn;4.9<br/>18.6&plusmn;4.7<br/>52.9&plusmn;6.0</td>
              <td>38.6&plusmn;5.9<br/>55.7&plusmn;6.0<br/>7.10&plusmn;3.1<br/>57.1&plusmn;6.0</td>
              <td class="uk-text-bold">58.6&plusmn;5.9<br/>88.6&plusmn;3.8<br/>24.3&plusmn;5.2<br/>70.0&plusmn;5.5</td>
            </tr>
          </tbody>
        </table>
      </div>

      |baselines||
      |--|--|
      | TRAJECTORY MATCHING | We utilize a standard trajectory matching as a baseline to evaluate our retrieval method on the basis of multi-agent skill representation. By calculating the similarity of the robot‚Äôs $xy$ -coordinate trajectories using FastDTW, we retrieve data from the $\mathcal D_{\text{prior}}$ that have trajectories similar to the target data‚Äôs trajectories.|
      | FEW-SHOT IMITATION LEARNING | We compare our method with a few-shot adaptation method. We trained the multi-task policy from $\mathcal D_{\text{prior}}$ and fine-tuned it using $\mathcal D_{\text{target}}$.|

      The Table shows our retrieval-augmented policy training outperforms few-shot imitation learning and agent-wise trajectory matching. These results indicate that our approach is more effective in tasks requiring advanced robot coordination, particularly in scenarios with a larger number of robots or in more complex tasks.

      ### Real-robot Experiments
      To validate the efficacy of our method in the real
      world, we use demonstrations of real wheeled robots for
      querying the prior dataset $D_{\text{prior}}$ constructed in simulated
      environments. We then train the policy using a few
      real-robot data augmented with the retrieved simulation
      demonstrations.
      <div class="uk-child-width-expand@m" uk-grid>
        <div class="">
          <img src="assets/exp_setting.png" alt="setting" />
        </div>
        <div class="">
          We employ our custom-designed swarm robot platform, <a href="https://github.com/omron-sinicx/swarm-body" class="uk-text-bold" target="_blank">``maru''</a>. The wheeled
          microrobots communicate with the host computer using 2.4
          GHz wireless communication. The positions of all robots
          and an object are tracked in real time by a high-speed digital
          light processing (DLP) structured light projector system.
          We can control the positions of the robots by sending 2D
          coordinates of their future locations from the host computer.
          We collect three demonstrations that naviate robots toward a given goal state using a hand-gesture system with Leap Motion, where the hands' movement is translated into the trajectories of multiple robots.
          Using the real-robot demonstrations as queries, we retrieve 300 simulated demonstrations from the prior dataset for each query.
        </div>
      </div>

      <div class="uk-child-width-expand@m" uk-grid>
        <div class="">
          <h5 class="uk-text-center">Query<br/>(Human Demonstration)</h5>
          <div class="video-container uk-align-center">
            <video src="assets/query.mp4" width="1800" height="1200" uk-video="autoplay: inview" loop muted playsinline></video>
            <canvas class="js-video-demo" width="320" height="200"></canvas>
          </div>
        </div>
        <div class="">
          <h5 class="uk-text-center">Baseline<br/>(No Retrieved Data)</h5>
          <div class="video-container uk-align-center">
            <video src="assets/baseline.mp4" width="1800" height="1200" uk-video="autoplay: inview" loop muted playsinline></video>
            <canvas class="js-video-demo" width="320" height="200"></canvas>
          </div>
        </div>
        <div class="">
          <h5 class="uk-text-center">Ours<br/>(Query+Retrieved)</h5>
          <div class="video-container uk-align-center">
            <video src="assets/ours.mp4" width="1800" height="1200" uk-video="autoplay: inview" loop muted playsinline></video>
            <canvas class="js-video-demo" width="320" height="200"></canvas>
          </div>
        </div>
      </div>

      We compare our policy trained using the real and retrieved demonstrations and the
      one trained using only the real-robot demonstrations. The
      results clearly demonstrate that the policy trained by our
      retrieve-and-learn framework successfully pushes an object
      closer to the goal state, while the policy trained using only
      a few real demonstrations (No Retrieved Data) fails to complete the task.
  - title: Acknowledgements
    text: |
      This work was supported by JST AIP Acceleration Research JPMJCR23U2, Japan.

projects:
  - title: 'maru: a miniature-sized wheeled robot for swarm robotics research'
    journal: "CHI'24"
    img: 'https://github.com/omron-sinicx/swarm-body/raw/main/images/teaser.jpg'
    description: |
      "maru" (= miniature assemblage adaptive robot unit) is a custom-made, miniature-sized, two-wheeled robot designed specifically for tabletop swarm robotics research.
    url: https://github.com/omron-sinicx/swarm-body
