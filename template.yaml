theme: default # default || dark
organization: OMRON SINIC X
twitter: '@omron_sinicx'
title: 'Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for Cooperative Manipulation by Mobile Robots'
conference: IROS2024
resources:
  paper: https://arxiv.org/abs/2312.02008
  code: # TBD
  video: # TBD
  blog: https://medium.com/sinicx/multipolar-multi-source-policy-aggregation-for-transfer-reinforcement-learning-between-diverse-bc42a152b0f5
  huggingface: # NA
description: introduces the Multi-Agent Coordination Skill Database, allowing multiple mobile robots to efficiently use past memories to adapt to new tasks.
image: https://omron-sinicx.github.io/mabr/assets/teaser.png
url: https://omron-sinicx.github.io/mabr
speakerdeck: # TBD
authors:
  - name: So Kuroki
    affiliation: [1, 2]
    url: http://barekatain.me/
    position: intern
  - name: Mai Nishimura
    affiliation: [1]
    position: Senior Researcher
    url: https://denkiwakame.github.io
  - name: Tadashi Kozuno
    affiliation: [1]
    position: Senior Researcher
    url: https://tadashik.github.io/
contact_ids: ['omron', 2] #=> github issues, contact@sinicx.com, 2nd author
affiliations:
  - OMRON SINIC X Corporation
  - Technical University of Munich
meta:
  - '* work done as an intern at OMRON SINIC X.'
bibtex: >
  @inproceedings{kuroki2024iros,
    title={Multi-Agent Behavior Retrieval: Retrieval-Augmented Policy Training for Cooperative Push Manipulation by Mobile Robots},
    author={So Kuroki, Mai Nishiura, Tadashi Kozuno},
    booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
    organization={IEEE}
    year={2024}
  }
header:
  bg_curve:
    sinic_curve.png
    #  bg_image: https://github.com/omron-sinicx/swarm-body/raw/main/images/teaser.jpg

teaser: teaser.png
overview: |
  Due to the complex interactions between agents, learning multi-agent control policy often requires a prohibited amount of data.
  This paper aims to enable multi-agent systems to effectively utilize past memories to adapt to novel collaborative tasks in a data-efficient fashion.
  We propose the *Multi-Agent Coordination Skill Database*, a repository for storing a collection of coordinated behaviors associated with key vectors distinctive to them. Our Transformer-based skill encoder effectively captures spatio-temporal interactions that contribute to coordination and provides a unique skill representation for each coordinated behavior. By leveraging only a small number of demonstrations of the target task, the database enables us to train the policy using a dataset augmented with the retrieved demonstrations. Experimental evaluations demonstrate that our method achieves a significantly higher success rate in push manipulation tasks compared with baseline methods like few-shot imitation learning. Furthermore, we validate the effectiveness of our retrieve-and-learn framework in a real environment using a team of wheeled robots.

body:
  - title: Multi-Agent Behavior Retrieval
    text: |
      ### Motivation
      When making decisions or engaging in collaborative tasks such as moving heavy loads or playing team sports, we effectively tap into past experiences by retrieving relevant memories. By drawing on the wealth of lessons learned from past successes and failures, we gain valuable insights that enhance our coordination ability to work together more efficiently even in unknown situations.
      In this work, we focus on endowing a team of mobile robots with such intelligent memory storage, which we refer to as a *MultiAgent Coordination Skill Database*. The database, which serves as a repository of coordination skills, efficiently stores collaborative demonstrations in diverse settings. Collecting a large number of multi-robot demonstrations for every target configuration is extremely costly and impractical in real-world scenarios. In contrast, by providing only a few demonstrations of the target task as queries, our database enables us to retrieve and learn from the vast repository of coordination skills that are relevant to the target task.

      ### Retrieve-and-Learn Framework
      Given a large task-agnostic prior dataset $D_{\text{prior}}$ and a few demonstrations $D_{\text{target}}$ collected from the target task, our main objective is to retrieve coordination skills from $D_{\text{prior}}$ that facilitate downstream policy learning for the target task.

      <img src="assets/method.png" alt="" />

      Our retrieve-and-learn framework consists of three primary components: (i) constructing the Multi-Agent Coordination
      Skill Database on the basis of the prior experiences, (ii) retrieving demonstrations using a few target demonstrations as queries,
      and (iii) learning the multi-agent control policy using the retrieved data and target data.

      ### Multi-Agent Coordination Skill Encoder
      We introduce a Transformer-based coordination skill encoder, which *learns to capture interactions among agents as well as interactions between agents and a manipulation object*.
      The design of the coordination skill encoder is guided
      by the following key insights.
      - The coordination occurs in both spatial and temporal domains. For example, if one agent maneuvers to the right side to contact an object, the other one must contact the left side of the object while predicting and interpreting the intention of the first agent.
      - The skill representation not only encompasses past trajectories but also predicts future trajectories. This enables
      the coordination skill to guide downstream policies, suggesting the appropriate action to take in future timesteps.

      To obtain such predictable skill representation space, we train
      the Transformer-based skill encoder as a prediction model of
      the agent‚Äôs future trajectories i.e., actions.
      <div class="uk-child-width-expand@s" uk-grid>
        <div class="">
          <img src="assets/skill_encoder.png" />
        </div>
        <div class="">
          In our network, the past context encoder is comprised of stacked multi-head self-attention layers that learn to attend to past trajectories across spatial and temporal domains. The future trajectory decoder is composed of multi-head self-attention layers and subsequent stacked multi-head cross-attention layers that integrate the past trajectory information and input tokens.
        </div>
      </div>
      Given past trajectory sequences with a temporal time window H, X =
  - title: Results
    text: |
      ### Quantitative Results on Simulated Demonstrations

      <div class="uk-overflow-auto">
        <table class="uk-table uk-table-small uk-text-small uk-table-divider">
          <thead>
            <tr>
              <th rowspan="2">Num of Agents</th>
              <th colspan="2" class="uk-text-center">task</th>
              <th colspan="3" class="uk-text-center">success rate ‚¨ÜÔ∏è [%]</th>
            </tr>
            <tr>
              <th>object</th>
              <th>level</th>
              <th>trajectory matching</th>
              <th>few-shot imitation learning</th>
              <th>‚ú®ours‚ú®</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>2</td>
              <td>üß±block<br/><br/>ü™Ñstick<br/><br/></td>
              <td><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span><br/><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span></td>
              <td><span class="uk-text-bold">41.4&plusmn;5.9</span><br/>55.7&plusmn;6.0<br/>32.9&plusmn;5.7<br/><span class="uk-text-bold">68.6&plusmn;5.6</span></td>
              <td>20.0&plusmn;4.8<br/>15.7&plusmn;4.4<br/>5.7&plusmn;2.8<br/>12.9&plusmn;4.0</td>
              <td><span class="uk-text-bold">41.4&plusmn;5.9<br/>57.1&plusmn;6.0<br/>35.7&plusmn;5.8</span><br/>67.1&plusmn;5.7</td>
            </tr>
            <tr>
              <td>3</td>
              <td>üß±block<br/><br/>ü™Ñstick<br/><br/></td>
              <td><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span><br/><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span></td>
              <td>32.9&plusmn;5.7<br/>34.3&plusmn;5.7<br/>34.3&plusmn;5.7<br/>52.9&plusmn;6.0</td>
              <td><span class="uk-text-bold">48.6&plusmn;6.0</span><br/>77.1&plusmn;5.1<br/>28.6&plusmn;5.4<br/><span class="uk-text-bold">67.1&plusmn;5.7</span></td>
              <td>42.9&plusmn;6.0<br/><span class="uk-text-bold">90.0&plusmn;3.6<br/>41.4&plusmn;5.9</span><br/>65.7&plusmn;5.7</td>
            </tr>
            <tr>
              <td>4</td>
              <td>üß±block<br/><br/>ü™Ñstick<br/><br/></td>
              <td><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span><br/><span class="uk-text-danger">hard</span><br/><span class="uk-text-success">easy</span></td>
              <td>55.7&plusmn;6.0<br/>78.6&plusmn;4.9<br/>18.6&plusmn;4.7<br/>52.9&plusmn;6.0</td>
              <td>38.6&plusmn;5.9<br/>55.7&plusmn;6.0<br/>7.10&plusmn;3.1<br/>57.1&plusmn;6.0</td>
              <td class="uk-text-bold">58.6&plusmn;5.9<br/>88.6&plusmn;3.8<br/>24.3&plusmn;5.2<br/>70.0&plusmn;5.5</td>
            </tr>
          </tbody>
        </table>
      </div>

      ### Real-robot Demonstration
      To validate the efficacy of our method in the real
      world, we use demonstrations of real wheeled robots for
      querying the prior dataset $D_{\text{prior}}$ constructed in simulated
      environments. We then train the policy using a few
      real-robot data augmented with the retrieved simulation
      demonstrations.
      <div class="uk-child-width-expand@s" uk-grid>
        <div class="">
          <img src="assets/exp_setting.png" alt="setting" />
        </div>
        <div class="">
          We employ our custom-designed swarm robot platform, <a href="https://github.com/omron-sinicx/swarm-body" class="uk-text-bold" target="_blank">``maru''</a> , where the wheeled
          microrobots communicate with the host computer using 2.4
          GHz wireless communication. The positions of all robots
          and an object are tracked in real time by a high-speed digital
          light processing (DLP) structured light projector system.
          We can control the positions of the robots by sending 2D
          coordinates of their future locations from the host compute
        </div>
      </div>
      <div class="uk-child-width-expand@m" uk-grid>
        <div class="">
          <h5 class="uk-text-center">Query<br/>(Human Demonstration)</h5>
          <div class="video-container uk-align-center">
            <video src="assets/query.mp4" width="1800" height="1200" uk-video="autoplay: inview" loop muted playsinline></video>
            <canvas class="js-video-demo" width="320" height="200"></canvas>
          </div>
        </div>
        <div class="">
          <h5 class="uk-text-center">Baseline<br/>(No Retrieved Data)</h5>
          <div class="video-container uk-align-center">
            <video src="assets/baseline.mp4" width="1800" height="1200" uk-video="autoplay: inview" loop muted playsinline></video>
            <canvas class="js-video-demo" width="320" height="200"></canvas>
          </div>
        </div>
        <div class="">
          <h5 class="uk-text-center">Ours<br/>(Query+Retrieved)</h5>
          <div class="video-container uk-align-center">
            <video src="assets/ours.mp4" width="1800" height="1200" uk-video="autoplay: inview" loop muted playsinline></video>
            <canvas class="js-video-demo" width="320" height="200"></canvas>
          </div>
        </div>
      </div>

      We compare our policy trained using the real and retrieved demonstrations and the
      one trained using only the real-robot demonstrations. The
      results clearly demonstrate that the policy trained by our
      retrieve-and-learn framework successfully pushes an object
      closer to the goal state, while the policy trained using only
      a few real demonstrations fails to complete the task.

projects:
  - title: 'maru: a miniature-sized wheeled robot for swarm robotics research'
    journal: "CHI'24"
    img: 'https://github.com/omron-sinicx/swarm-body/raw/main/images/teaser.jpg'
    description: |
      "maru" (= miniature assemblage adaptive robot unit) is a custom-made, miniature-sized, two-wheeled robot designed specifically for tabletop swarm robotics research.
    url: https://github.com/omron-sinicx/swarm-body
